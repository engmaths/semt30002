[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scientific Computing and Optimisation",
    "section": "",
    "text": "This unit will bring together your previous experience of solving mathematical problems (optimisation, differential equations) and writing computer programmes using Python, Matlab, or Julia. The aim is to provide you with a suite of tools that can be used to transform real-world problems, such as those encountered in the Mathematical and Data Modelling units and final-year projects, into problems that can be efficiently solved on a computer.\n\nUnit timetable\n\nWeek 1: Finite differences and Euler’s method\nWeek 2: ODE boundary value problems\nWeek 3: Diffusion equations in 1D\nWeek 4: First-order PDEs in 1D\nWeek 5: The 2D Poisson equation\nWeek 6: Reading week (no sessions)\nWeek 7: Gradient-based optimisation\nWeek 8: Gradient-free optimisation\nWeek 9: Linear programming\nWeek 10: Integer linear programming\nWeek 11: Optimisation extras and coursework support\nWeek 12: No sessions\n\n\n\nTeaching sessions\n\nMonday 1200–1300 - demo and Q&A session - MVB 1.15\nThursday 1600–1800 - lab session – QB 1.19 (Weeks 1 and 2 only) then MVB 1.15\nFriday 1300–1400 - drop-in (optional) - MVB 1.15\n\n\n\nSoftware\nYou can code in Python, Matlab, or Julia. The choice is yours. However, some demos and examples will only be provided in one programming language.\nBefore the unit begins, please ensure that you have a functioning installation of your programming language of choice.\nIf you are coding in Python, then you will need the following packages:\n\nSciPy and NumPy\nMatplotlib\nmemory_profiler\nPyomo\n\nIf you are coding in Matlab, then you will need the optimisation and global optimisation toolboxes.\nIf you are coding in Julia, then you will need the following packages:\n\nOrdinaryDiffEq.jl\nA plotting package (e.g., GLMakie.jl or Plots.jl)\nBenchmarkTools.jl\nJuMP.jl\nAlso see modern Julia workflows\n\nIn Julia these can be installed from the REPL (compilation will take a few minutes):\nimport Pkg\nPkg.add([\"OrdinaryDiffEq\", \"GLMakie\", \"BenchmarkTools\", \"JuMP\"])\n\n\nTeaching staff\n\nDr Matthew Hennessy (unit director): matthew.hennessy@bristol.ac.uk\nProf David Barton (unit lecturer): david.barton@bristol.ac.uk\nMr Roussel Desmond Nzoyem (teaching assistant): rd.nzoyemngueguin@bristol.ac.uk"
  },
  {
    "objectID": "optim/week7.html",
    "href": "optim/week7.html",
    "title": "Week 7: Gradient-based optimisation",
    "section": "",
    "text": "In the following exercises you will need to develop some (simple) code to implement gradient-based optimisation algorithms. You are free to use any resources available to develop your codes, but I recommend trying to understand the details of how they work to build up intuition for later in the course.\nMost optimisation toolboxes only implement the more sophisticated methods (e.g., Adam, BFGS, Conjugate Gradient, etc). These can be good for checking your results and performance comparisons.\n\n\n\nAlgorithms for Optimisation - Local descent (line search methods)\nAlgorithms for Optimisation - First-order gradient\nVarious optimisation algorithm implementations\n\nPython - scipy.optimize\nJulia - Optim.jl\nMATLAB - Optimization Toolbox\n\n\n\n\n\nConsider the minimisation of the objective function \\[\n    f(x,y)=2x^2+5y^2-2xy -2x-8y\n\\] of two unconstrained variables \\(x\\) and \\(y\\).\n\nWorking by hand. Freeze \\(x=0\\), and find the location \\(y^*\\) of the minimum of the function \\(f(0,y)\\) of one variable \\(y\\). Then freeze \\(y=y^*\\) and find the location \\(x^*\\) of the minimum of the function \\(f(x,y^*)\\) of one variable \\(x\\).\nWorking by hand. Find \\(\\nabla f\\). Thus, beginning from the origin \\((0,0)\\), perform one step of the steepest-descent algorithm. That, is find the \\(\\alpha^{(1)}\\) such that the function \\((0,0)^\\mathrm{T}+\\alpha\\nabla f(0,0)\\) of one variable \\(\\alpha\\) is minimised, and find the corresponding coordinates of the next iterate \\((x^{(1)},y^{(1)})\\).\nUse your favourite package to plot (e.g., contours) of \\(f(x,y)\\) in the quadrant \\(x_1,x_2\\geq0\\) and overlay points and lines corresponding to your part 1. and 2. answers.\nDevelop, exhibit, and explain gradient descent code that finds the minimum value and the location of the minimum of \\(f(x,y)\\) above. (You can write out the gradient by hand rather than using finite differences.) Plot your results on the contour plot from part 3. For stepsize selection, try each of:\n\nUse a fixed step size \\(\\alpha\\) of your choice.\nUse a decaying step size \\(\\alpha^{(k)} = \\alpha^{(1)}\\gamma^{k-1}\\) for \\(\\gamma=0.99\\).\nUse a backtracking line search.\n\n\n\n\n\nThe Rosenbrock function is a standard test case for many optimisation algorithms because it is ill conditioned (i.e., nasty): \\[\n    f(x,y) = (a - x)^2 + b(y - x^2)^2\n\\] where typically \\(a = 1\\) and \\(b = 100\\). The gradient of the Rosenbrock function is The gradient of the Rosenbrock function is given by\n\\[\n\\nabla f(x,y) =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x} \\\\\n\\frac{\\partial f}{\\partial y}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-2(a - x) - 4bx(y - x^2) \\\\\n2b(y - x^2)\n\\end{bmatrix}.\n\\]\n\nImplement a backtracking line search so that it can be applied to any objective function and optimisation method. Assume that you have the gradient a I.e., backtracking(rosenbrock, grad_rosenbrock, gradientdescent, [-1.5,-0.5]) should use the backtracking algorithm with the basic gradient descent optimisation algorithm and apply it to the Rosenbrock function. Here rosenbrock and grad_rosenbrock are functions of a single variable \\(u=[x,y]\\) that return the objective function and its gradient, respectively. The function gradientdescent provides the search direction given by the gradient descent algorithm.\nNOTE: all algorithms other than gradient descent require additional values to be stored (e.g., momentum). Your implementation should be able to deal with this; a nice solution would be something like\ndef momentum(objfunc, stepsize, position, extra):\n    if extra is None:  # create a new vector to store momentum\n        extra = np.zeros_like(position)\n    # Calculate new position based on the stepsize given by the line search\n    # ...\n    # Return the new position and updated momentum vector\n    return (position, extra)\nImplement each of the following optimisation algorithms in a form that works with your backtracking line search from Q1.\n\nGradient descent (basic algorithm)\nGradient descent with momentum\nRMSProp\nAdam\n\nTry each of the algorithms on the Rosenbrock function. Compare the number of iterations required for convergence for each algorithm. What conclusions can you draw from this?\n\n\n\n\nBelow are three additional objective functions commonly used to test the performance of gradient-based optimisation algorithms. Use \\(n=2\\) in each case. (Feel free to test the performance in more dimensions!)\nApply your code from exercise 2 to each of these test cases. Visualise the results.\n\n\nThe Sphere function is a simple convex function used for testing optimisation algorithms\n\\[\nf(x) = \\sum_{i=1}^{n} x_i^2.\n\\]\n\n\n\nThe Rastrigin function is a non-convex function used to test the performance of optimisation algorithms on multimodal landscapes\n\\[\nf(x) = 10n + \\sum_{i=1}^{n} \\left[ x_i^2 - 10 \\cos(2 \\pi x_i) \\right].\n\\]\n\n\n\nThe Ackley function is another non-convex function with many local minima, making it a challenging test case for optimisation algorithms\n\\[\nf(x) = -20 \\exp \\left( -0.2 \\sqrt{0.5 \\sum_{i=1}^{n} x_i^2} \\right) - \\exp \\left( 0.5 \\sum_{i=1}^{n} \\cos(2 \\pi x_i) \\right) + \\exp(1) + 20.\n\\]"
  },
  {
    "objectID": "optim/week7.html#supplementary-material",
    "href": "optim/week7.html#supplementary-material",
    "title": "Week 7: Gradient-based optimisation",
    "section": "",
    "text": "Algorithms for Optimisation - Local descent (line search methods)\nAlgorithms for Optimisation - First-order gradient\nVarious optimisation algorithm implementations\n\nPython - scipy.optimize\nJulia - Optim.jl\nMATLAB - Optimization Toolbox"
  },
  {
    "objectID": "optim/week7.html#exercise-1",
    "href": "optim/week7.html#exercise-1",
    "title": "Week 7: Gradient-based optimisation",
    "section": "",
    "text": "Consider the minimisation of the objective function \\[\n    f(x,y)=2x^2+5y^2-2xy -2x-8y\n\\] of two unconstrained variables \\(x\\) and \\(y\\).\n\nWorking by hand. Freeze \\(x=0\\), and find the location \\(y^*\\) of the minimum of the function \\(f(0,y)\\) of one variable \\(y\\). Then freeze \\(y=y^*\\) and find the location \\(x^*\\) of the minimum of the function \\(f(x,y^*)\\) of one variable \\(x\\).\nWorking by hand. Find \\(\\nabla f\\). Thus, beginning from the origin \\((0,0)\\), perform one step of the steepest-descent algorithm. That, is find the \\(\\alpha^{(1)}\\) such that the function \\((0,0)^\\mathrm{T}+\\alpha\\nabla f(0,0)\\) of one variable \\(\\alpha\\) is minimised, and find the corresponding coordinates of the next iterate \\((x^{(1)},y^{(1)})\\).\nUse your favourite package to plot (e.g., contours) of \\(f(x,y)\\) in the quadrant \\(x_1,x_2\\geq0\\) and overlay points and lines corresponding to your part 1. and 2. answers.\nDevelop, exhibit, and explain gradient descent code that finds the minimum value and the location of the minimum of \\(f(x,y)\\) above. (You can write out the gradient by hand rather than using finite differences.) Plot your results on the contour plot from part 3. For stepsize selection, try each of:\n\nUse a fixed step size \\(\\alpha\\) of your choice.\nUse a decaying step size \\(\\alpha^{(k)} = \\alpha^{(1)}\\gamma^{k-1}\\) for \\(\\gamma=0.99\\).\nUse a backtracking line search."
  },
  {
    "objectID": "optim/week7.html#exercise-2",
    "href": "optim/week7.html#exercise-2",
    "title": "Week 7: Gradient-based optimisation",
    "section": "",
    "text": "The Rosenbrock function is a standard test case for many optimisation algorithms because it is ill conditioned (i.e., nasty): \\[\n    f(x,y) = (a - x)^2 + b(y - x^2)^2\n\\] where typically \\(a = 1\\) and \\(b = 100\\). The gradient of the Rosenbrock function is The gradient of the Rosenbrock function is given by\n\\[\n\\nabla f(x,y) =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x} \\\\\n\\frac{\\partial f}{\\partial y}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-2(a - x) - 4bx(y - x^2) \\\\\n2b(y - x^2)\n\\end{bmatrix}.\n\\]\n\nImplement a backtracking line search so that it can be applied to any objective function and optimisation method. Assume that you have the gradient a I.e., backtracking(rosenbrock, grad_rosenbrock, gradientdescent, [-1.5,-0.5]) should use the backtracking algorithm with the basic gradient descent optimisation algorithm and apply it to the Rosenbrock function. Here rosenbrock and grad_rosenbrock are functions of a single variable \\(u=[x,y]\\) that return the objective function and its gradient, respectively. The function gradientdescent provides the search direction given by the gradient descent algorithm.\nNOTE: all algorithms other than gradient descent require additional values to be stored (e.g., momentum). Your implementation should be able to deal with this; a nice solution would be something like\ndef momentum(objfunc, stepsize, position, extra):\n    if extra is None:  # create a new vector to store momentum\n        extra = np.zeros_like(position)\n    # Calculate new position based on the stepsize given by the line search\n    # ...\n    # Return the new position and updated momentum vector\n    return (position, extra)\nImplement each of the following optimisation algorithms in a form that works with your backtracking line search from Q1.\n\nGradient descent (basic algorithm)\nGradient descent with momentum\nRMSProp\nAdam\n\nTry each of the algorithms on the Rosenbrock function. Compare the number of iterations required for convergence for each algorithm. What conclusions can you draw from this?"
  },
  {
    "objectID": "optim/week7.html#exercise-3-extension",
    "href": "optim/week7.html#exercise-3-extension",
    "title": "Week 7: Gradient-based optimisation",
    "section": "",
    "text": "Below are three additional objective functions commonly used to test the performance of gradient-based optimisation algorithms. Use \\(n=2\\) in each case. (Feel free to test the performance in more dimensions!)\nApply your code from exercise 2 to each of these test cases. Visualise the results.\n\n\nThe Sphere function is a simple convex function used for testing optimisation algorithms\n\\[\nf(x) = \\sum_{i=1}^{n} x_i^2.\n\\]\n\n\n\nThe Rastrigin function is a non-convex function used to test the performance of optimisation algorithms on multimodal landscapes\n\\[\nf(x) = 10n + \\sum_{i=1}^{n} \\left[ x_i^2 - 10 \\cos(2 \\pi x_i) \\right].\n\\]\n\n\n\nThe Ackley function is another non-convex function with many local minima, making it a challenging test case for optimisation algorithms\n\\[\nf(x) = -20 \\exp \\left( -0.2 \\sqrt{0.5 \\sum_{i=1}^{n} x_i^2} \\right) - \\exp \\left( 0.5 \\sum_{i=1}^{n} \\cos(2 \\pi x_i) \\right) + \\exp(1) + 20.\n\\]"
  },
  {
    "objectID": "scicomp/week1.html",
    "href": "scicomp/week1.html",
    "title": "Week 1: Finite differences and Euler’s method",
    "section": "",
    "text": "\\[\n\\renewcommand{\\vec}[1]{\\boldsymbol{#1}}\n\\newcommand{\\td}[2]{\\frac{\\mathrm{d}#1}{\\mathrm{d}#2}}\n\\newcommand{\\tdd}[2]{\\frac{\\mathrm{d}^2#1}{\\mathrm{d}#2^2}}\n\\newcommand{\\pd}[2]{\\frac{\\partial#1}{\\partial#2}}\n\\newcommand{\\pdd}[2]{\\frac{\\partial^2#1}{\\partial#2^2}}\n\\]\n\nOverview\nTo start off the unit, we dive into finite differences. This is a general method for approximating derivatives of mathematical functions with algebraic formulae. This will be a crucial step when it comes to deriving numerical methods for solving ODEs and PDEs. We also introduce the idea of truncation error, explain why this is useful, and how it can be calculated using Taylor expansions. Finally, we recap Euler’s method, a simple yet useful approach for numerically approximating the solution of time-dependent ODEs.\n\n\nSupplementary material\n\nPDF of demos\nNotes on the derivation of finite-difference formulae using Taylor expansions\n\n\n\nExercises\n\nConsider the following finite-difference approximation of the second derivative \\[\n\\left.\\tdd{u}{x}\\right|_{x=a} = \\frac{2 u(a) + C u(a+\\Delta x) + 4u(a + 2 \\Delta x) - u(a + 3\\Delta x)}{(\\Delta x)^2}\n\\] First, take \\(C = 5\\). By calculating the truncation error analytically or numerically, determine whether this is a valid approximation formula. If the formula is valid, determine the order of the approximation. Is the approximation formula valid when \\(C = -5\\)? If so, what is the order of the approximation?\nImplement a version of Euler’s method that can solve an arbitrary system of ODEs of the form \\[\n\\td{\\vec{u}}{t} = \\vec{f}(\\vec{u}, t, \\mu),\n\\] where \\(\\mu\\) represents one or many parameters that the function \\(\\boldsymbol{f}\\) might depend on. The initial condition is \\(\\boldsymbol{u}(0) = \\boldsymbol{u}_0\\).\nValidate your implementation of Euler’s method by numerically solving the system of ODEs given by \\[\n\\td{u_1}{t} = -\\omega u_2, \\qquad\n\\td{u_2}{t} = \\omega u_1, \\tag{1}\n\\] over the time range \\(0 \\leq t \\leq 2 \\pi\\). The initial condition can be set to \\(u_1(0) = 1\\) and \\(u_2(0) = 0\\). The exact solution to this problem is given by \\(u_1(t) = \\cos(\\omega t)\\) and \\(u_2(t) = \\sin (\\omega t)\\).\n\nSet \\(\\omega = 1\\) and use \\(N_t = 100\\) time steps to find a numerical solution. Plot \\(u_1\\) as a function of time using your numerical solution and the exact solution.\n\nKeeping \\(\\omega = 1\\), create similar plots when \\(N_t\\) is decreased to 50 and increased to 1000. From the plots, what happens to the accuracy of the numerical solution as \\(N_t\\) increases?\n\nFor the ODE system given by (1), use numerical experimentation to determine how different values of \\(\\omega\\) impact the solution. If \\(u_1\\) represents the displacement of a mass on a spring, then what does \\(\\omega\\) physically correspond to?\nFor the ODE system given by (1), compute the error between your numerical solution and the exact solution at time \\(t = 2 \\pi\\); that is, calculate \\(\\varepsilon = |u_1^{N_t} - 1|\\), where \\(u_1^{N_t}\\) is the numerical solution at time step \\(N_t\\).\n\nFor a fixed value of \\(\\omega = 1\\), how fast does the error decrease with \\(\\Delta t\\)? Is it linear, quadratic, etc? Is this what you expect based on the truncation error of Euler’s method?\n\nFor a fixed number of time steps, \\(N_t = 10^4\\), how does the error change as \\(\\omega\\) increases? Can you provide a justification of the results you see? For this problem, try setting \\(\\omega = 1, 2, 4, 8, 16\\).\n\n\n\n\nCoursework-style problem\nConsider a pack of \\(N\\) identical lithium-ion batteries that are connected in series. Let \\(i = 1\\) denote the first battery in the pack and \\(i = N\\) denote the last battery in the pack. The other batteries are labelled with \\(i = 2, 3, \\ldots, N - 1\\). Due to the geometry of the pack, only batteries 1 and \\(N\\) are in contact with the environment. Battery \\(i\\) is in contact with batteries \\(i-1\\) and \\(i + 1\\).\nEach battery in the pack acts as a resistor. When an electrical current is drawn from the battery pack, each battery will heat up due to Joule heating. The heat that is generated in battery \\(i\\) will be transferred to batteries \\(i-1\\) and \\(i + 1\\), causing these two batteries to heat up even more. Heat is transferred from one battery to the next until it reaches batteries 1 and \\(N\\), which can then transfer the heat to the air. The more batteries there are, the longer the generated heat remains in the battery pack, and the hotter the pack will get. If the temperature of any battery exceeds 60 \\(^\\circ\\)C, then it will start to degrade. Battery degradation is extremely dangerous, as this can lead to the release of toxic and explosive gases.\nThe aim of this exercise is to simulate the temperature of a battery pack in order to determine whether it is expected to be safe to use.\nAssume that a constant current \\(I\\) is being drawn from the battery pack. If \\(T_i(t)\\) denotes the temperature of battery \\(i\\) at time \\(t\\), then the temperature of each of the batteries can be obtained by solving the ODE system given by \\[\\begin{align}\nc \\td{T_1}{t} &= I^2 R - h(T_{1} - T_\\text{air}) - h(T_{1} - T_{2}), \\tag{2} \\\\\nc \\td{T_i}{t} &= I^2 R - h(T_{i} - T_{i-1}) - h(T_{i} - T_{i+1}), \\quad i = 2, 3, \\ldots, N - 1, \\tag{3} \\\\\nc \\td{T_N}{t} &= I^2 R - h(T_{N} - T_\\text{air}) - h(T_{N} - T_{N-1}). \\tag{4}\n\\end{align}\\] Here, \\(c\\) is the specific heat of the battery, \\(R\\) is the resistance of the battery, \\(h\\) is the heat transfer coefficient, and \\(T_\\text{air}\\) is the temperature of the air. Values for these parameters are given in the table below. You can assume that the initial temperature of each battery is given by \\(T_\\text{air}\\); that is, \\(T_i(0) = T_\\text{air}\\).\n\nParameter values for each battery.\n\n\n\n\n\n\n\n\n\n\\(I\\) (A)\n\\(c\\) (J/\\(^\\circ\\)C)\n\\(h\\) (W/\\(^\\circ\\)C)\n\\(T_\\text{air}\\) (\\(^\\circ\\)C)\n\\(R\\) (Ohms)\n\n\n\n\n3\n300\n0.7\n23\n0.4\n\n\n\nIn the exercises below, assume that the current is being drawn for one hour.\n\nConsider a battery pack with \\(N = 5\\) batteries.\n\nNumerically solve the ODE system in (2)–(4). Create a single plot that shows the temperature of each battery as a function of time.\nAt each time \\(t\\), compute the maximum temperature across all 6 batteries. Plot the maximum temperature as a function of time.\nUse your results to determine whether the battery is safe to use. Answer: yes.\n\nIs a battery pack with \\(N = 50\\) batteries safe to use? Answer: no.\nWhat is the largest number of batteries that a pack can contain yet still be safe to use? Answer: 11."
  },
  {
    "objectID": "scicomp/timing_code.html",
    "href": "scicomp/timing_code.html",
    "title": "Timing code in Python and Matlab",
    "section": "",
    "text": "Python\nA simple way to time a piece of code in Python is to use the timeit package:\nimport timeit\nThen, write a Python function that contains all of the code that you want to time. For example,\ndef timed_code():\n    A = np.eye(5000)\n    b = np.random.random(5000)\n    x = np.linalg.solve(A, b)\nThe next step is to call the timeit function from the timeit package as follows:\ntime = timeit.timeit(lambda: timed_code(), number = 1)\nThis will run the timed_code function one time. The time in seconds will then be stored in the Python variable time, which can then be printed to the screen.\nFor fast functions, more accurate timings can be obtained by running the function multiple times. This is possible by increasing the value of the number argument. The time that is returned by timeit will the total time. Dividing the total time by the number of times the function was called with give an accurate average value.\nYou can learn more about the timeit package here\n\n\nMatlab\nTiming code in Matlab is very easy; this can be done using the tic and toc functions. For example, in the command window you can type\ntic \nA = eye(5000)\nb = rand(5000, 1)\nx = A \\ b\ntoc\nAfter the toc function is called, the elapsed time since tic was called will be printed to the screen. It’s usually better to write the code that you want to time in a function and then call that function in between the tic and toc.\nYou can find out more about timing code in Matlab by reading its official page on the topic."
  },
  {
    "objectID": "scicomp/week2.html",
    "href": "scicomp/week2.html",
    "title": "Week 2: ODE BVPs",
    "section": "",
    "text": "\\[\n\\renewcommand{\\vec}[1]{\\boldsymbol{#1}}\n\\newcommand{\\td}[2]{\\frac{\\mathrm{d}#1}{\\mathrm{d}#2}}\n\\newcommand{\\tdd}[2]{\\frac{\\mathrm{d}^2#1}{\\mathrm{d}#2^2}}\n\\newcommand{\\pd}[2]{\\frac{\\partial#1}{\\partial#2}}\n\\newcommand{\\pdd}[2]{\\frac{\\partial^2#1}{\\partial#2^2}}\n\\]\n\nOverview\nThis week showcase how finite difference methods can be used to solve boundary value problems (BVPs) for ODEs. In short, the finite difference method converts ODEs into algebraic systems of equations, which can then be solved using two main approaches:\n\nWriting your own solvers using linear algebra functions.\nUsing built-in solvers provided by Python (SciPy) and Matlab.\n\nThe code that you develop this week will form an essential role in the upcoming weeks when the focus shifts towards numerically solving partial differential equations.\n\n\nSupplementary material\n\nPDF of demos\n\n\n\nExercises\nThe goal of the exercises is to create a BVP solver that is capable of finding numerical solutions to ODEs of the form \\[\nD \\tdd{u}{x} + q(x, u; \\mu) = 0,\n\\] where the domain of the problem is given by \\(a \\leq x \\leq b\\). In this equation, \\(D &gt; 0\\) is a parameter. The term \\(q(x, u; \\mu)\\) represents a function that depends on the spatial coordinate \\(x\\), the solution \\(u(x)\\), and a parameter \\(\\mu\\). Your solver should be able to handle all three types of boundary conditions (Dirichlet, Neumann, and Robin).\nBuilding this solver is a big task. The exercises below are designed so that you can see how to build your code by solving a sequence of simpler problems. Test your code extensively and only add complexity once you are certain it works correctly.\n\nUse finite differences to find a numerical solution to \\[\n\\tdd{u}{x} = 0, \\quad\nu(a) = \\gamma_1, \\quad u(b) = \\gamma_2.\n\\] In this case, the exact solution to the problem is given by \\[\nu(x) =  \\left(\\frac{\\gamma_2 - \\gamma_1}{b-a}\\right)(x - a) + \\gamma_1, \\tag{1}\n\\] which you can use to test your code.\nExtend your code so that it can account for a source term in the ODE: \\[\nD \\tdd{u}{x} + q(x) = 0, \\quad\nu(a) = \\gamma_1, \\quad u(b) = \\gamma_2.\n\\] Hint: The simplest place to start is to set \\(q(x) = 1\\); in this case, the exact solution is given by \\[\nu(x) = -\\frac{1}{2D}(x-a)(x-b) +  \\left(\\frac{\\gamma_2 - \\gamma_1}{b-a}\\right)(x - a) + \\gamma_1.\n\\] Once you’ve developed code for the problem with \\(q(x) = 1\\), add an \\(x\\) dependence into \\(q\\). Can you find some exact solutions for this case?\nGeneralise your code so that the source term \\(q\\) can now depend on the solution \\(u\\) as well as a parameter \\(\\mu\\). Use your code to solve the equation \\[\n\\tdd{\\phi}{x} + \\mu(\\phi - \\phi^3) = 0\n\\] with the boundary conditions \\(\\phi(-1) = -1\\) and \\(\\phi(1) = 1\\) when (a) \\(\\mu = 0.5\\) and (b) \\(\\mu = 100\\).\nHint 1: If \\(q\\) depends nonlinearly on the solution, as with this problem, then a good initial guess of the solution is usually required for the nonlinear solver to converge. For this problem, a good initial guess can be found by noting that when \\(\\mu\\) is small, the source term \\(q\\) is approximately zero. The solution in this case is given by (1), which can be used to form an initial guess.\nHint 2: If you find it difficult to obtain a solution when \\(\\mu = 100\\), then try to increase \\(\\mu\\) in increments from \\(0.5\\) to \\(100\\). Each time a new solution is successfully computed, use it as the initial guess for the next (larger) value of \\(\\mu\\). This process of gradually ramping up a parameter and using the previously computed solution as the next initial guess is called natural parameter continuation.\n(Optional) Solve the previous exercise using your own implementation of Newton’s method and a built-in algebraic solver (e.g. root or fsolve). Compare the performance of the solver. What solver is more likely to converge? Answer: you should see that Newton’s method is more likely to converge (assuming you’re using the default options with the built-in solver).\nUpdate your code so that it can account for any combination of Dirichlet, Neumann, and Robin boundary conditions. Remember to validate your code against problems with known solutions!\nTry using your code to solve the problem \\[\n\\tdd{u}{x} + 1 = 0, \\quad \\left.\\td{u}{x}\\right|_{x=\\pm 1} = 0.\n\\] To understand what happens in this case, try solving this problem by hand. Alternatively, integrate the ODE and impose the boundary conditions."
  },
  {
    "objectID": "scicomp/poisson_example.html",
    "href": "scicomp/poisson_example.html",
    "title": "Example code for the 2D Poisson equation",
    "section": "",
    "text": "\\[\n\\renewcommand{\\vec}[1]{\\boldsymbol{#1}}\n\\newcommand{\\td}[2]{\\frac{\\mathrm{d}#1}{\\mathrm{d}#2}}\n\\newcommand{\\tdd}[2]{\\frac{\\mathrm{d}^2#1}{\\mathrm{d}#2^2}}\n\\newcommand{\\pd}[2]{\\frac{\\partial#1}{\\partial#2}}\n\\newcommand{\\pdd}[2]{\\frac{\\partial^2#1}{\\partial#2^2}}\n\\]\n\nOverview\nThe code below solves the Poisson equation \\[\n\\pdd{u}{x} + \\pdd{u}{y} + q = 0\n\\] where \\(q\\) is a constant on a rectangular domain given by \\(a \\leq x \\leq b\\) and \\(c \\leq y \\leq d\\). Dirichlet boundary conditions given by \\(u = 0\\) are imposed at all four boundaries.\n\n\nCode\n\nimport numpy as np\nfrom scipy.optimize import root\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\n\"\"\" \nset up the problem parameters\n\"\"\"\n\n# domain parameters\na = 0\nb = 1\nc = 0\nd = 1\n\n# number of grid points\nNx = 20\nNy = 20\n\n# Value of the constant source term\nq = 1\n\n\"\"\"\nconstruct the grid    \n\"\"\"\n\n# grid points\nx = np.linspace(0, 1, Nx+1)\ny = np.linspace(0, 1, Ny+1)\nx_int = x[1:Nx]\ny_int = y[1:Ny]\n\n# grid spacing in x and y directions\ndx = (b - a) / Nx\ndy = (d - c) / Ny\n\n# total number of unknowns\ndof = (Nx - 1) * (Ny - 1)\nprint('There are', dof, 'unknowns to solve for')\n\n# mapping from grid indices (i,j) to global indices (k)\nk = lambda i,j : i + (Nx - 1) * j\n\n\"\"\"\nFunction to pass to SciPy's root.  This function\nbuilds the algebraic system in the form F(U) = 0,\nwhere U is a 1D array that contains the solution\ncomponents at all interior grid points.  The code\nbelow is not optimal and improvements can be made.\n\"\"\"\n\ndef dirichlet_problem(U):\n\n    # Pre-allocation of 2D arrays\n    u = np.zeros((Nx-1, Ny-1))\n    F = np.zeros((Nx-1, Ny-1))\n\n    # Convert the 1D soln array U[k] into a 2D array u[i,j]\n    for i in range(0, Nx - 1):\n        for j in range(0, Ny - 1):\n            u[i,j] = U[k(i,j)]\n    \n    # Build the algebraic system as a 2D array F[i,j]\n    for i in range(0, Nx - 1):\n        for j in range(0, Ny - 1):\n\n            # near x = a boundary\n            if i == 0 and 0 &lt; j &lt; Ny - 2:\n                F[i,j] = (\n                    (u[i+1,j] - 2 * u[i,j] + 0) / dx**2 + \n                    (u[i,j+1] - 2 * u[i,j] + u[i,j-1]) / dy**2 + \n                    q\n                )\n\n            # near x = b boundary\n            elif i == Nx - 2 and 0 &lt; j &lt; Ny - 2:\n                F[i,j] = (\n                    (0 - 2 * u[i,j] + u[i-1,j]) / dx**2 + \n                    (u[i,j+1] - 2 * u[i,j] + u[i,j-1]) / dy**2 + \n                    q\n                )\n                \n            # near y = c boundary\n            elif j == 0 and 0 &lt; i &lt; Nx - 2:\n                F[i,j] = (\n                    (u[i+1,j] - 2 * u[i,j] + u[i-1,j]) / dx**2 + \n                    (u[i,j+1] - 2 * u[i,j] + 0) / dy**2 + \n                    q\n                )\n\n            # near y = d boundary\n            elif j == Ny - 2 and 0 &lt; i &lt; Nx - 2:\n                F[i,j] = (\n                    (u[i+1,j] - 2 * u[i,j] + u[i-1,j]) / dx**2 + \n                    (0 - 2 * u[i,j] + u[i,j-1]) / dy**2 + \n                    q\n                )\n\n            # near x = a, y = c corner\n            elif i == 0 and j == 0:\n                F[i,j] = (\n                    (u[i+1,j] - 2 * u[i,j] + 0) / dx**2 + \n                    (u[i,j+1] - 2 * u[i,j] + 0) / dy**2 + \n                    q\n                )\n\n            # near x = a, y = d corner\n            elif i == 0 and j == Ny - 2:\n                F[i,j] = (\n                    (u[i+1,j] - 2 * u[i,j] + 0) / dx**2 + \n                    (0 - 2 * u[i,j] + u[i,j-1]) / dy**2 + \n                    q\n                )\n\n            # near x = b, y = c corner\n            elif i == Nx - 2 and j == 0:\n                F[i,j] = (\n                    (0 - 2 * u[i,j] + u[i-1,j]) / dx**2 + \n                    (u[i,j+1] - 2 * u[i,j] + 0) / dy**2 + \n                    q\n                )\n\n            # near x = b, y = d corner\n            elif i == Nx - 2 and j == Ny - 2:\n                F[i,j] = (\n                    (0 - 2 * u[i,j] + u[i-1,j]) / dx**2 + \n                    (0 - 2 * u[i,j] + u[i,j-1]) / dy**2 + \n                    q\n                )\n\n            # grid points not adjacent to a boundary\n            else:\n                F[i,j] = (\n                    (u[i+1,j] - 2 * u[i,j] + u[i-1,j]) / dx**2 + \n                    (u[i,j+1] - 2 * u[i,j] + u[i,j-1]) / dy**2 + \n                    q\n                )\n\n    # Now the 2D array for F[i,j] needs to be converted into a\n    # 1D array of the form F[k]\n    F_1d = np.zeros((Nx-1) * (Ny-1))\n    for i in range(Nx-1):\n        for j in range(Ny-1):\n            F_1d[k(i,j)] = F[i,j]\n\n    \n    # return the 1D array\n    return F_1d\n\n\n\"\"\"\nsolve the algebraic system using SciPy's root function\n\"\"\"\n\n# First we set the initial guess (in this case zeros)\nU_0 = np.zeros((Nx-1) * (Ny-1))\n\n# Solve\nsol = root(dirichlet_problem, U_0)\n\n# Check for convergence\nprint(f'Did root converge: {sol.success}')\nU = sol.x\n\n\"\"\"\nturn the 1D solution array U into a 2D array u\n\"\"\"\nu = np.zeros((Nx-1, Ny-1))\nfor i in range(Nx-1):\n    for j in range(Ny-1):\n        u[i,j] = U[k(i,j)]\n\n\n\"\"\"\nnow we plot the solution\n\"\"\"\n\n# turn the 1D arrays for x_int and y_int\n# into 2D arrays for plotting\nxx, yy, = np.meshgrid(x_int, y_int)\n\n# due to how the global index function k(i,j) is defined\n# we need to plot the transpose of u rather than u\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nax.plot_surface(xx, yy, u.T, cmap=cm.coolwarm)\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"u\")\nplt.show()\n\nThere are 361 unknowns to solve for\nDid root converge: True"
  },
  {
    "objectID": "python/python_review.html",
    "href": "python/python_review.html",
    "title": "Python fundamentals",
    "section": "",
    "text": "Overview\n\nTo provide an overview of core Python functionality and programming techniques\nRefresh your memory of Python syntax\nThis page only covers the basics and is by no means exhaustive.\n\n\n\nBasic variable types\n\nInts: integers; e.g. a = 2\nFloats: floating-point numbers with decimals; e.g. a = 2.0\nStrings: collection of characters contained in single or double quotes; individual characters can be accessed using an index (starting at 0)\n\n\na = 2\nb = 2.0\ns = \"hello\"\nprint(s[0])\n\nh\n\n\nUse the int, float, and str functions to convert between types\n\na = 2.00\nprint(int(a))\n\n2\n\n\n\n\nMathematical operations\n\n\n\nOperation\nDescription\nExample\n\n\n\n\n+\nAddition\n5 + 3 = 8\n\n\n-\nSubstraction\n5 - 3 = 2\n\n\n*\nMultiplication\n5 * 3 = 15\n\n\n/\nDivision\n5 / 3 = 1.666…\n\n\n//\nFloor division (round down to an integer)\n5 // 3 = 1\n\n\n%\nModulo (compute remainder)\n5 % 3 = 2\n\n\n**\nExponent\n5 ** 3 = 125\n\n\n\n\n\nBoolean operations\n\n\n\nOperation\nDescription\nExample\nValue\n\n\n\n\n==\nIs equal?\n1 == 2\nFalse\n\n\n!=\nIs not equal?\n1 != 2\nTrue\n\n\n&lt;\nLess than?\n1 &lt; 2\nTrue\n\n\n&gt;\nGreater than?\n1 &gt; 2\nFalse\n\n\n&lt;=\nLess than or equal to?\n1 &lt;= 2\nTrue\n\n\n&gt;=\nGreater than or equal to?\n1 &gt;= 2\nFalse\n\n\n\n\n\nLogical operations\n\n\n\nOperation\nDescription\nExample\nValue\n\n\n\n\nand\nAre both true?\n1 &lt; 2 and 3 &lt; 2\nFalse\n\n\nor\nIs one true?\n1 &lt; 2 or 3 &lt; 2\nTrue\n\n\nnot\nNegate the conditional\nnot(1 &lt; 2)\nFalse\n\n\n\n\n\nData structures\n\n\n\nType\nExample\nCharacteristics\n\n\n\n\nList\nL = [1, 1.0, ‘one’]\nMutable, iterable, ordered\n\n\nTuple\nt = (1, 1.0, ‘one’)\nImmutable, iterable, ordered\n\n\nSet\ns = {1, 1.0, ‘one’}\nMutable, iterable, unordered, unique\n\n\nDictionary\nd = {‘a’:1, ‘b’:2, ‘c’:3}\nMutable, iterable, ordered\n\n\n\n\nMutable: Can be modified\nImmutable: Cannot be modified\nOrdered: Elements can be accessed using an index or a key\n\n\n\nData structures continued\n\nUse list, tuple, and set functions to convert between types\nElements in lists and tuples can be accessed using an integer index (starting at 0)\nElements in dictionaries are accessed using keys\n\n\n# create a list and print the first value\nL = [1, 2, 3]\nprint(L[0])\n\n1\n\n\n\n# create a dictionary of gravitational accelerations in m/s2\ng = {'Earth': 9.8, 'Mars':3.7, 'Jupiter':25}\nprint(g['Earth'])\n\n9.8\n\n\n\n\nIf statements\n\nUsed to make a decision in a program\nRuns an indented block of code if a conditional statement is true\n\n\ni = 20\n\nif i &lt; 10:\n    print(\"Doing something because i &lt; 10 and the code is indented\")\n    \nprint('Printing non-indented code for all values of i')\n\nPrinting non-indented code for all values of i\n\n\n\n\nIf-else statements\n\nCreates two pathways, the choice depends on whether a condition is true or false\n\n\ni = 20\n\nif i &lt; 10:\n    print('Doing something because i &lt; 10')\nelse:\n    print('Doing something else i &gt;= 10')\n\nDoing something else i &gt;= 10\n\n\n\n\nIf-else-elif statements\n\nCreates multiple pathways, the choice depends on which condition is true\n\n\ni = 20\n\nif i &lt; 10:\n    print('Doing something because i &lt; 10')\nelif i &gt; 10:\n    print('Doing something else because i &gt; 10')\nelse:\n    print('Doing something different from the other two cases')\n\nDoing something else because i &gt; 10\n\n\n\n\nFor loops\n\nFor repeating code a fixed number of times\n\nfor e in collection:\n    # run indented code\n\nThe indented code is run until e has taken on every value in collection (which is an iterable object like a list or tuple)\n\n\n# print the numbers from 0 to 10\nfor i in range(11):\n    print(i, end=\" \")\n\n0 1 2 3 4 5 6 7 8 9 10 \n\n\n\n# capitalise words in a list\nL = ['red', 'blue', 'green']\nfor c in L:\n    print(c.capitalize(), end=\", \")\n\nRed, Blue, Green, \n\n\n\n\nWhile loops\n\nFor repeating code until a condition becomes false\n\nwhile condition:\n    # run indented code\n\nWhile loops are useful when you don’t know how many times to repeat code\nBeware of infinite loops!\n\n\n# compute the square numbers that are smaller than 450\nn = 1\n\nwhile n**2 &lt; 450:\n    print(n**2, end=\", \")\n    n += 1\n\n1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, \n\n\n\n\nBreak and continue\n\nbreak is used to terminate a loop\ncontinue is used to skip an iteration in a loop\n\n\nfor i in range(10):\n    print(i, end = \" \")\n\n0 1 2 3 4 5 6 7 8 9 \n\n\n\nfor i in range(10):\n    if i == 4:\n        break\n    print(i, end = \" \")\n\n0 1 2 3 \n\n\n\nfor i in range(10):\n    if i == 4:\n        continue\n    print(i, end = \" \")\n\n0 1 2 3 5 6 7 8 9 \n\n\n\n\nFunctions\n\nFunctions are mini-programs based on a collection of code that has been given a name\nFunctions are defined using the def keyword\nFunction inputs are called arguments\nThe return keyword is used to output data from a function\n\n\n# add two numbers a and b together\ndef my_sum(a, b):\n    c = a + b\n    return c\n\nc = my_sum(3, 6)\nprint(c)\n\n9\n\n\n\n\nScripts, modules, and packages\n\nModules are Python files (.py) that contain variables, functions, etc\nPackages are folders (directories) that contain modules\nScripts are top-level Python files that import packages and modules\nScripts are run (e.g. in Spyder) not modules/packages\n\nA typical file structure might look like this:\nemat30008/\n|--- main.py\n|--- circle.py\nwhere main.py is a script that imports the module circle.py\n\n\nImporting modules and packages\n\nThe import keyword is used to load Python code from modules and packages\nThere are many ways to do this; see EMAT10007 notes for more details\n\n\n# import the math package\nimport math\n\n# print the variable pi from the math package\nprint(math.pi)\n\n3.141592653589793\n\n\n\n\nSummary\nThese slides covered core Python functionality\n\nOperations, data types, control flow, loops, functions, modules and packages\n\nTopics not covered but which you are expected to know:\n\nVariable scope (local, global), keyword and default arguments, classes, file input and output"
  },
  {
    "objectID": "assessment.html",
    "href": "assessment.html",
    "title": "Assessment",
    "section": "",
    "text": "This unit will be assessed by a single piece of coursework that is to be completed individually. It will be released in two parts, one in week 5 and one in week 9. Details of the coursework will be released on Blackboard.\nThe final deadline is 12 noon on Friday of week 11. Submit your coursework as a .zip file on Blackboard."
  },
  {
    "objectID": "python/numpy.html",
    "href": "python/numpy.html",
    "title": "NumPy, SciPy, and Matplotlib",
    "section": "",
    "text": "NumPy\n\nNumPy is a Python library that enables vectors and matrices to be stored as arrays\nNumPy provides very fast mathematical functions that can operate on these arrays.\n\n\n\nImporting NumPy\nIt is common to import NumPy using the command\n\nimport numpy as np\n\n\n\nDefining arrays\n\nArrays are defined using the array function.\nA vector (1D array) can be created by passing a list to array\n\nExample: Create the vector \\(v = (1, 2, 3)\\)\n\nv = np.array([1, 2, 3])\nprint(v)\n\n[1 2 3]\n\n\nA matrix (2D array) can be created by passing a nested list to array, where each inner list is a row of the matrix\nExample: Create the matrix \\[\nM = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}\n\\]\n\nM = np.array([ [1, 2], [3, 4] ])\nprint(M)\n\n[[1 2]\n [3 4]]\n\n\n\n\nAccessing elements\n\nIndividual elements in a 1D array can be accessed using square brackets and a numerical index\nIndexing NumPy arrays starts at 0\n\n\n# print the second element of vector v\nprint(v[1])\n\n2\n\n\n\nUse two indices separated by a comma for 2D arrays (first index = row, second index = column)\n\n\n# print the entry in the second row, first column of M\nprint(M[1, 0])\n\n3\n\n\n\n\nAccessing sequential elements\nA colon (:) can be used to access sequential elements in an array:\n\nv = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\nprint(v[:])\n\n[1 2 3 4 5 6 7 8 9]\n\n\nThe notation v[a:b] will access entries starting at index \\(a\\) and ending at \\(b-1\\)\n\n# print the third to fifth entries\nprint(v[2:5])\n\n[3 4 5]\n\n\n\n\nSome useful functions for creating arrays\n\nlinspace(a, b, N) creates a 1D array with \\(N\\) uniformly spaced entries between \\(a\\) and \\(b\\) (inclusive)\neye(N) creates the \\(N \\times N\\) identity matrix\nones(dims) creates arrays filled with ones, where dims is a tuple of integers that describes the dimensions of the array\nzeros(dims) creates arrays filled with zeros\nrandom.random(dims) creates an array with random numbers between 0 and 1 from a uniform distribution\n\n\n\nOperations on NumPy arrays\nMany mathematical operations can be performed immediately\n\n+ and -: element-by-element addition and subtraction\n*: scalar multiplication or element-by-element multiplication\ndot(a,b): dot product of two 1D arrays a and b\n@: matrix multiplication\n\nNumPy comes with mathematical functions that can operate on arrays (e.g. trig functions, exp, log) * np.sin(x): applies the sin function to each element of x\n\n\nLinear algebra with NumPy\nThe linalg module of NumPy has functions for linear algebra. For example:\n\nlinalg.solve(A,b): Solve a linear system of equations of the form \\(Ax = b\\)\nlinalg.det(A): Compute determinants of \\(A\\)\nlinalg.inv(A): Compute the inverse of \\(A\\), ie \\(A^{-1}\\)\nlinalg.eig(A): Compute the eigenvalues and eigenvectors of \\(A\\)\n\n\n\nSciPy\nIs a Python package that contains functions for a wide range of mathematical problems\n\nSpecial functions, e.g. Bessel functions\nSolving nonlinear equations\nOptimisation\nInterpolation\nIntegration (including solving ODEs)\nLinear algebra (including sparse linear algebra)\nand more\n\nThe SciPy package is imported using the code\n\nimport scipy\n\nAs part of this unit, we will be solving nonlinear algebraic equations and optimisation problems\n\nscipy.optimize.root solves algebraic equations\nscipy.optimise.minimize minimises a scalar function with multiple variables\n\nWe will also learn about other SciPy functions that are useful for finding the numerical solution to PDEs and optimisation problems.\n\n\nMatplotlib\n\nUsed for visualising data in Python (eg creating plots)\nWorks well with NumPy\n\nUsually imported using\n\nimport matplotlib.pyplot as plt\n\n\n\nA basic example\nPlot \\(y = \\sin(x)\\) from \\(x = 0\\) to \\(x = 2\\pi\\)\n\nx = np.linspace(0, 2 * np.pi, 100)\ny = np.sin(x)\n\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nThere are many options that can edited to make figures look nicer\nThere are also many different styles of figures (e.g. contour plots, scatter plots)\nSee https://github.com/rougier/matplotlib-tutorial for a good overview of the options\n\n\n# use latex fonts and use a fontsize of 16 everywhere\nplt.rcParams.update({\"text.usetex\": True, \"font.size\": 16})\n\n# plot\nplt.plot(x, y, linewidth=2, color='black')\n\n# add labels to the axes\nplt.xlabel(r'$x$')\nplt.ylabel(r'$\\sin(x)$')\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSummary\n\nNumPy provides functionality for storing numerical data as arrays and performing operations on these\nSciPy contains functions for solving a wide variety of mathematical problems\nMatplotlib is for visualising data"
  },
  {
    "objectID": "python/overview.html",
    "href": "python/overview.html",
    "title": "Recap of Python programming",
    "section": "",
    "text": "Use the links below to refresh your knowledge of coding in Python:\n\nOverview of Python fundamentals\nOverview of NumPy, SciPy, and Matplotlib"
  },
  {
    "objectID": "scicomp/week3.html",
    "href": "scicomp/week3.html",
    "title": "Week 3: Diffusion equations in 1D",
    "section": "",
    "text": "\\[\n\\renewcommand{\\vec}[1]{\\boldsymbol{#1}}\n\\newcommand{\\td}[2]{\\frac{\\mathrm{d}#1}{\\mathrm{d}#2}}\n\\newcommand{\\tdd}[2]{\\frac{\\mathrm{d}^2#1}{\\mathrm{d}#2^2}}\n\\newcommand{\\pd}[2]{\\frac{\\partial#1}{\\partial#2}}\n\\newcommand{\\pdd}[2]{\\frac{\\partial^2#1}{\\partial#2^2}}\n\\]\n\nOverview\nThis week we enter the realm of numerically solving partial differential equations (PDEs). We will focus on one of the most important PDEs of them all: the diffusion equation. By using finite differences to discretise space, the diffusion equation can be converted into a system of ODEs. There are two main ways to solve this ODE system.\nThe explicit Euler method is the simplest method, and it can be extended to nonlinear problems relatively easily. However, the method only works if the time step \\(\\Delta t\\) is sufficiently small. This means that a large number of time steps may be required to obtain a solution.\nThe implicit Euler method is more complex because it involves solving a system of algebraic equations at each time step. Howver, larger values of the time step \\(\\Delta t\\) are usually possible. In fact, for the linear diffusion equations, the implicit Euler method is unconditionally stable, meaning it will work for any time step size \\(\\Delta t\\). The need to solve an algebraic system of equations at each time step comes at a high computational cost. Hence, there is a tradeoff between being able to take fewer time steps to obtain a numerical solution and the higher computational cost per time step.\n\n\nSupplementary material\nUse the links below to find:\n\nPDF of demos (updated 29/09)\nSome solutions to the diffusion equation\nHow to time Python and Matlab code\n\n\n\nExercises\nThe goal of this week is to develop code that can solve reaction-diffusion equations of the form \\[\n\\pd{u}{t} = D \\pdd{u}{x} + q(x, t, u; \\mu), \\tag{1}\n\\] where the diffusion coefficient \\(D &gt; 0\\). The domain of the problem is given by \\(a \\leq x \\leq b\\). The term \\(q(x, t, u; \\mu)\\) represents a function that depends on the spatial coordinate \\(x\\), time \\(t\\), the solution \\(u(x, t)\\), and a parameter \\(\\mu\\).\nYour code should be able to handle all combinations of boundary conditions. You should implement both the explicit and implicit Euler method.\n\nUse the explicit Euler method to solve the linear diffusion equation without a source term \\[\n\\pd{u}{t} = D\\pdd{u}{x}.\n\\] Set the boundary conditions to \\(u(a,t) = 0\\) and \\(u(b,t) = 0\\) and the initial condition to \\[\nu(x,0) = \\sin\\left(\\frac{\\pi (x - a)}{b - a}\\right).\n\\]\nThe exact solution to the problem is given by \\[\nu(x, t) =  \\exp\\left(-\\frac{D \\pi^2 t}{(b-a)^2}\\right)\\sin\\left(\\frac{\\pi (x - a)}{b - a}\\right).\n\\] which you can use to test your code.\nUse the implicit Euler method to solve the problem in Exercise 1, using the same analytical solution to validate your code.\nExplicit vs implicit Euler. In this exercise, you will compare the performance of the two Euler methods by timing your codes. Find out more about how to time your code here. Consider the diffusion equation and boundary conditions from Exercise 1. Take \\(a = 0\\), \\(b = 1\\), and \\(D = 0.1\\). Discretise the spatial domain using 31 grid points (\\(N = 30\\)) and solve the problem until time \\(T = 1\\).\n\nFix the time step to be \\(\\Delta t = 0.005\\). Show that this time step is smaller than the maximum time step that can be used with the explicit Euler method. Solve the problem using the explicit and implicit Euler method. Which method is the fastest and why?\nIncrease the time step to \\(\\Delta t = 0.1\\) and solve with the implicit Euler method. How does the speed of the implicit Euler method now compare to the speed of the explicit Euler method with \\(\\Delta t = 0.005\\)? Which of the two methods leads to a more accurate solution when \\(t = 1\\)?\n\nExtend your code so that it can solve linear reaction-diffusion equations of the form \\[\n\\pd{u}{t} = D \\pdd{u}{x} + q(x), \\\\\n\\] with two Dirichlet boundary conditions. Use the supplementary notes to find exact solutions that you can use to test your code with.\nSolve the Fisher-KPP equation (a nonlinear reaction-diffusion equation) given by \\[\n\\pd{u}{t} = D\\pdd{u}{x} + r u (1 - u)\n\\] on the domain \\(0 \\leq x \\leq 10\\). The boundary and initial conditions are \\[\nu(0,t) = 1, \\qquad u(10,t) = 0, \\qquad u(x,0) = 0.\n\\] You can take \\(D = 0.1\\) and \\(r = 2\\). Run your simulation until \\(t = 20\\) and use 51 grid points (\\(N = 50\\)). What types of solution do you see?\nHint: It may help to code up an explicit Euler method for this problem first due to its simplicity. Then, implement an implicit Euler method, and validate the results by comparing with your explicit Euler method.\nExtend your code so that it can handle Neumann and Robin boundary conditions. One way to start this process is to solve \\[\n\\pd{u}{t} = D\\pdd{u}{x}\n\\] with boundary conditions given by \\[\n\\quad u(0,t) = 0, \\quad D\\left.\\pd{u}{x}\\right|_{x=L} = j,  \\quad u(x,0) = 0.\n\\] Hint: To validate your code, you can find an exact solution to this problem using the separation of variables. Alternatively, you can take \\(L \\gg 1\\) (e.g. \\(L = 10\\)) and use the similarity solution in the supplementary notes, which gives \\(u(L,t) \\simeq j(4 t / \\pi D)^{1/2}\\) for sufficiently small values of \\(t\\)."
  },
  {
    "objectID": "scicomp/week5.html",
    "href": "scicomp/week5.html",
    "title": "Week 5: The 2D Poisson equation",
    "section": "",
    "text": "\\[\n\\renewcommand{\\vec}[1]{\\boldsymbol{#1}}\n\\newcommand{\\td}[2]{\\frac{\\mathrm{d}#1}{\\mathrm{d}#2}}\n\\newcommand{\\tdd}[2]{\\frac{\\mathrm{d}^2#1}{\\mathrm{d}#2^2}}\n\\newcommand{\\pd}[2]{\\frac{\\partial#1}{\\partial#2}}\n\\newcommand{\\pdd}[2]{\\frac{\\partial^2#1}{\\partial#2^2}}\n\\]\n\nOverview\nThis week, we will use finite differences to solve PDEs in two dimensions. Particular focus will be placed on solving Poisson’s equation. For 2D problems, the number of unknowns and hence the size of the linear systems quickly grow as the number of grid points is increased. Sparse matrices, which only store the non-zero elements of a matrix, can lead to substantial reductions in the memory that needed to numerically solve the PDE, especially for 2D problems.\n\n\nSupplementary material\n\nExample code for the Poisson equation\nPDF of demos\n\n\n\nExercises\nFor the exercises below, you can use the example code for solving Poisson’s equation as a starting point. Alternatively, you can build your own code from scratch to solve the problem using linear algebra functions.\n\nSolve the Poisson equation \\[\n\\nabla^2 u + 2 = 0\n\\] on the domain \\(0 \\leq x \\leq 1\\) and \\(1 \\leq y \\leq 4\\). Assume that \\(u = 0\\) on the boundaries.\nUse memory profiling to determine how much memory is required to solve the problem in Exercise 1. How does the memory usage depend on the number of grid points?\nGeneralise your code from Exercise 1 so that it can solve problems of the form \\[\nD \\nabla^2 u + q(x,y) = 0.\n\\] To validate your code, set \\[\nq(x,y) = D \\pi^2 \\left[\\frac{1}{(b-a)^2} + \\frac{1}{(d-c)^2}\\right]\\sin\\left[\\frac{\\pi(x-a)}{b-a}\\right]\\sin\\left[\\frac{\\pi(y-c)}{d-c}\\right];\n\\] in this case, the exact solution to the problem is given by \\[\nu(x,y) = \\sin\\left[\\frac{\\pi(x-a)}{b-a}\\right]\\sin\\left[\\frac{\\pi(y-c)}{d-c}\\right].\n\\]\nFurther generalise your code so that non-homogeneous Dirichlet boundary conditions can be used. For example, solve the Poisson equation for Exercise 1 but replace the boundary condition at \\(x = a\\) with \\(u(a, y) = \\sin(\\pi(y-c)/(d-c))\\).\nUse sparse matrices to solve the 1D Poisson equation given by \\[\n\\tdd{u}{x} + 1 = 0\n\\] with \\(u(0) = u(1) = 0\\).\n\nFirst use 101 grid points (\\(N = 100\\)). Time your code and profile its memory usage. Is your sparse code faster and more memory efficient than your previous code?\nRepeat part (a) but now use \\(N = 1000\\). If you have correctly implemented sparse matrices, then you should now see a significant speed up and memory reduction.\n\nSolve the 2D Poisson equation with sparse matrices."
  },
  {
    "objectID": "scicomp/week4.html",
    "href": "scicomp/week4.html",
    "title": "Week 4: First-order PDEs in 1D",
    "section": "",
    "text": "\\[\n\\renewcommand{\\vec}[1]{\\boldsymbol{#1}}\n\\newcommand{\\td}[2]{\\frac{\\mathrm{d}#1}{\\mathrm{d}#2}}\n\\newcommand{\\tdd}[2]{\\frac{\\mathrm{d}^2#1}{\\mathrm{d}#2^2}}\n\\newcommand{\\pd}[2]{\\frac{\\partial#1}{\\partial#2}}\n\\newcommand{\\pdd}[2]{\\frac{\\partial^2#1}{\\partial#2^2}}\n\\]\n\nOverview\nThis week we focus on an first-order PDEs. These are PDEs that only involve first-order partial derivatives. Although these are the simplest of all PDEs, they lead to surprising numerical challenges. In particular, the finite-difference discretisation needs to adapt to the sign of the coefficient in front of the spatial derivative (the wave speed). Choosing a numerically stable discretisation requires a consideration of the direction of wave propagation. Adapting the spatial discretisation based on the direction of wave propagation is called upwinding.\nIn this unit, we will use the upwind scheme to numerically solve first-order PDEs. Although this method is simple, it suffers from numerical diffusion. This leads to the numerical solution artifically spreading out. Moreover, there is also a restriction on the time step that is set by the CFL condition.\n\n\nSupplementary material\n\nPDF of demos\n\n\n\nExercises\nThe goal of this week is to develop code that can solve reaction-advection equations of the form \\[\n\\pd{u}{t} + v(x,t,u) \\pd{u}{x} = q(x, t, u; \\mu), \\tag{1}\n\\] where the speed \\(v\\) can be positive or negative. The domain of the problem is given by \\(a \\leq x \\leq b\\). The term \\(q(x, t, u; \\mu)\\) represents a function that depends on the spatial coordinate \\(x\\), time \\(t\\), the solution \\(u(x, t)\\), and a parameter \\(\\mu\\).\n\nSolve the linear advection equation given by \\[\n\\pd{u}{t} + v \\pd{u}{x} = 0\n\\] where \\(v\\) is a constant. Take the domain to be \\(0 \\leq x \\leq 5\\) and \\(0 \\leq t \\leq 4\\). Set the initial condition to \\(u(x,0) = x / 5\\).\n\nSolve with \\(v = 1\\) and the boundary condition \\(u(0, t) = 0\\). Validate your code using the exact solution \\[\nu(x,t) = \\begin{cases}\n0, \\quad &0 \\leq x \\leq t, \\\\\n\\frac{1}{5}(x - t), \\quad &t \\leq x \\leq 5.\n\\end{cases}\n\\]\nSolve with \\(v = -1\\) and the boundary condition \\(u(5, t) = 1\\). Validate your code using the exact solution \\[\nu(x,t) = \\begin{cases}\n\\frac{1}{5}(x + t), \\quad &0 \\leq x \\leq 5 - t, \\\\\n1, \\quad &5 - t \\leq x \\leq 5.\n\\end{cases}\n\\]\nUse the velocity decomposition to write an upwind scheme that can simultaneously handle both of these cases.\n\nThe upwind scheme has a truncation error that is \\(O(\\Delta x)\\) in space and \\(O(\\Delta t)\\) in time. Show that your code produces solutions with the expected accuracy. Hint: One way to do this is to consider the problem in exercise 1a. Compare your numerical approximation to \\(u(5,4)\\) to the exact value of \\(1/5\\) by fixing \\(\\Delta t\\) and varying \\(\\Delta x\\) and vice-versa.\nConsider again the problem in exercise 1a. Set \\(N = 100\\) so that \\(\\Delta x\\) is now fixed. Now choose values of \\(\\Delta t\\) so that the CFL number is set to \\(C = 0.5\\), \\(C = 0.75\\), and \\(C = 1\\). On the same axes, plot \\(u(x,4)\\) for all three cases along with the exact solution. What happens when \\(C\\) is increased to one? How you can explain the results in term of the truncation error and the numerical diffusion coefficient?\nSolve the linear advection equation given by \\[\n\\pd{u}{t} + v(x,t)\\pd{u}{x} = 0\n\\] on the domain \\(-1 \\leq x \\leq 1\\). Let the velocity be given by \\(v(x, t) = x\\). Notice that \\(v &lt; 0\\) for \\(x &lt; 0\\) and \\(v &gt; 0\\) for \\(x &gt; 0\\). Take the initial condition to be \\(u(x,0) = 1 - x^2\\). Do boundary conditions need to be imposed for this problem?\nSolve the hyperbolic conservation law \\[\n\\pd{u}{t} + \\pd{}{x}\\left(j(u)\\right) = 0\n\\] when \\(j(u) = u + u^3 / 3\\). Take the domain to be \\(-2 \\leq x \\leq 5\\) and \\(0 \\leq t \\leq 2\\). The initial condition can be set to \\(u(x,0) = \\exp(-x^2)\\). If needed, set \\(u = 0\\) at the boundaries.\nSolve the forced inviscid Burgers’ equation \\[\n\\pd{u}{t} + u \\pd{u}{x} = 1\n\\] over the domain \\(-\\pi \\leq x \\leq \\pi\\) and \\(0 \\leq t \\leq 2\\). The initial condition can be set to \\(u(x,0) = \\sin x\\). If boundary conditions are needed, set \\(u = t\\) at the boundary."
  },
  {
    "objectID": "scicomp/overview.html",
    "href": "scicomp/overview.html",
    "title": "Scientific Computing: overview",
    "section": "",
    "text": "The first five weeks will focus on how scientific computing can be applied to the numerical solution of ordinary and partial differential equations. Although differential equations appear in many engineering contexts, they usually cannot be solved by hand. Instead, numerical methods are needed to compute an approximate solution.\n\nWeek 1: Finite differences and Euler’s method\nWeek 2: ODE boundary value problems\nWeek 3: Diffusion equations in 1D\nWeek 4: First-order PDEs in 1D\nWeek 5: The 2D Poisson equation"
  },
  {
    "objectID": "optim/overview.html",
    "href": "optim/overview.html",
    "title": "Scientific Computing: Overview",
    "section": "",
    "text": "The weeks 7-11 will focus on optimisation algorithms and their use in scientific computing. We will cover a range of numerical algorithms including gradient-based methods (e.g., Adam - used for training neural networks), gradient-free methods (e.g., Nelder-Mead and genetic algorithms), linear programming (often used for optimising business processes), and (mixed) integer linear programming.\n\nWeek 7: Gradient-based optimisation"
  }
]